{"version":3,"sources":["index.js"],"names":["__webpack_require__","r","__webpack_exports__","console","time","expressionArray","require","setBackend","faceapi","imageEnrol","document","getElementById","Promise","all","nets","faceRecognitionNet","loadFromUri","faceLandmark68TinyNet","tinyFaceDetector","faceExpressionNet","then","i","result","xmlhttp","XMLHttpRequest","open","concat","send","status","responseText","descriptions","descriptorArray","JSON","parse","x","push","floatArray","Float32Array","from","labeledFaceDescriptors","LabeledFaceDescriptors","faceMatcher","FaceMatcher","rand","Math","floor","random","log","timeEnd","addEventListener","image","bufferToImage","files","img","detection","detectSingleFace","TinyFaceDetectorOptions","withFaceLandmarks","withFaceExpressions","withFaceDescriptor","dtn","sorted","expressions","asSortedArray","expression","probability","findBestMatch","descriptor","distance","alert"],"mappings":"4HAAAA,EAAAC,EAAAC,GAAAF,EAAA,KAAAA,EAAA,KAIAG,QAAQC,KAAK,WAEb,IAAMC,EAAkB,CAAC,UAAW,QAAS,MAAO,QAAS,UAAW,YAAa,aAE1EC,EAAQ,KAChBC,WAAW,OACd,IAAMC,EAAUF,EAAQ,KAClBG,EAAaC,SAASC,eAAe,eAG3CC,QAAQC,IAAI,CACVL,EAAQM,KAAKC,mBAAmBC,YAAY,YAC5CR,EAAQM,KAAKG,sBAAsBD,YAAY,YAC/CR,EAAQM,KAAKI,iBAAiBF,YAAY,YAC1CR,EAAQM,KAAKK,kBAAkBH,YAAY,cAC1CI,KAEH,WAEE,IAAK,IAAIC,EAAI,EAAGA,GAAK,EAAGA,IAAK,CAC3B,IACIC,EADAC,EAAU,IAAIC,eAElBD,EAAQE,KAAK,MAAb,iBAAAC,OAAqCL,EAArC,UAA+C,GAC/CE,EAAQI,OACY,KAAhBJ,EAAQK,SACVN,EAASC,EAAQM,cAEnB,IAAIC,EAAe,GACfC,EAAkBC,KAAKC,MAAMX,GACjC,IAAK,IAAIY,KAAKH,EACZD,EAAaK,KAAKJ,EAAgBG,IAEpC,IAAIE,EAAaC,aAAaC,KAAKR,IACnCA,EAAe,IACFK,KAAKC,GAEpB,IAAMG,EAAyB,IAAI/B,EAAQgC,uBAAuB,OAAQV,GACpEW,EAAc,IAAIjC,EAAQkC,YAAYH,EAAwB,IAEhEI,EAAOC,KAAKC,MAAsB,EAAhBD,KAAKE,UAC3B3C,QAAQ4C,IAAI,sDAAwD1C,EAAgBsC,IACpFxC,QAAQ6C,QAAQ,WAChB7C,QAAQ4C,IAAI,SAEZtC,EAAWwC,iBAAiB,SAAU,WAEpC9C,QAAQC,KAAK,YAEb,IAAI8C,EAAQ1C,EAAQ2C,cAAc1C,EAAW2C,MAAM,IAEnDF,EAAM9B,KAAK,SAACiC,GAEVlD,QAAQ4C,IAAI,gBAEZ,IAAIO,EAAY9C,EAAQ+C,iBAAiBF,EAAK,IAAI7C,EAAQgD,yBACzDC,mBAAkB,GAAMC,sBAAsBC,qBAE/CL,EAAUlC,KAAK,SAACwC,GAEd,IACIC,EAASD,EAAIE,YAAYC,gBAE7B,GADA5D,QAAQ4C,IAAIc,EAAO,GAAGG,YAClBH,EAAO,GAAGI,aAHQ,IAGwBJ,EAAO,GAAGG,aAAe3D,EAAgBsC,GACvF,CACE,IAAMrB,EAASmB,EAAYyB,cAAcN,EAAIO,YACvCC,EAAW9C,EAAM,SACnB8C,GAAY,IAEdjE,QAAQ6C,QAAQ,YAChBqB,MAAM,kBAINlE,QAAQ6C,QAAQ,YAChBqB,MAAM,wCAKRlE,QAAQ6C,QAAQ,YAChBqB,MAAM","file":"static/js/main.2499e016.chunk.js","sourcesContent":["// import l10n.js first\nimport 'kaios-gaia-l10n';\nimport './index.css';\n\nconsole.time(\"LOADING\")\n\nconst expressionArray = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgusted', 'surprised'];\n\nconst tf = require('@tensorflow/tfjs')\ntf.setBackend('cpu')\nconst faceapi = require('@vladmandic/face-api/dist/face-api.node-cpu.js')\nconst imageEnrol = document.getElementById('imageUpload');\n\n\nPromise.all([\n  faceapi.nets.faceRecognitionNet.loadFromUri('./models'),\n  faceapi.nets.faceLandmark68TinyNet.loadFromUri('./models'),\n  faceapi.nets.tinyFaceDetector.loadFromUri('./models'),\n  faceapi.nets.faceExpressionNet.loadFromUri('./models')\n]).then(start);\n\nfunction start() {\n\n  for (let i = 1; i <= 3; i++) {\n    var xmlhttp = new XMLHttpRequest();\n    var result;\n    xmlhttp.open(\"GET\", `./descriptors/${i}.json`, false);\n    xmlhttp.send();\n    if (xmlhttp.status==200) {\n      result = xmlhttp.responseText;\n    }\n    var descriptions = []\n    var descriptorArray = JSON.parse(result)\n    for (let x in descriptorArray) {\n      descriptions.push(descriptorArray[x])\n    }\n    var floatArray = Float32Array.from(descriptions)\n    descriptions = []\n    descriptions.push(floatArray)\n  }\n  const labeledFaceDescriptors = new faceapi.LabeledFaceDescriptors('john', descriptions)\n  const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.4)\n\n  var rand = Math.floor(Math.random() * 7)\n  console.log(\"Please upload an image with the following emotion: \" + expressionArray[rand])\n  console.timeEnd(\"LOADING\")\n  console.log('Ready')\n\n  imageEnrol.addEventListener('change', () => {\n\n    console.time(\"MATCHING\")\n\n    var image = faceapi.bufferToImage(imageEnrol.files[0])\n    \n    image.then((img) => {\n\n      console.log('Image loaded')\n\n      var detection = faceapi.detectSingleFace(img, new faceapi.TinyFaceDetectorOptions())\n      .withFaceLandmarks(true).withFaceExpressions().withFaceDescriptor()\n\n      detection.then((dtn) => {\n\n        const minConfidence = 0.4\n        var sorted = dtn.expressions.asSortedArray()\n        console.log(sorted[0].expression)\n        if (sorted[0].probability >= minConfidence && sorted[0].expression === expressionArray[rand])\n        {\n          const result = faceMatcher.findBestMatch(dtn.descriptor)\n          const distance = result['distance']\n          if (distance <= 0.4) \n          {\n            console.timeEnd(\"MATCHING\")\n            alert('You are John')\n          } \n          else\n          {\n            console.timeEnd(\"MATCHING\")\n            alert('you are not John. \\ntry again.')\n          }\n        }\n        else \n        {\n          console.timeEnd(\"MATCHING\")\n          alert(\"Liveness Test Failed\")\n        }\n      });\n    });\n  })\n}\n"],"sourceRoot":""}