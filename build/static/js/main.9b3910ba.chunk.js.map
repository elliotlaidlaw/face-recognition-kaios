{"version":3,"sources":["index.js"],"names":["__webpack_require__","r","__webpack_exports__","require","setBackend","faceapi","Promise","all","nets","faceRecognitionNet","loadFromUri","faceLandmark68TinyNet","faceExpressionNet","tinyFaceDetector","then","window","localStorage","getItem","imageEnrol","document","getElementById","console","log","addEventListener","image","bufferToImage","files","img","detection","detectSingleFace","TinyFaceDetectorOptions","withFaceLandmarks","withFaceDescriptor","dtn","descriptor","data","JSON","stringify","myStorage","setItem","location","reload","enrol","imageUpload","expressionArray","result","descriptions","descriptorArray","parse","x","push","floatArray","Float32Array","from","labeledFaceDescriptors","LabeledFaceDescriptors","faceMatcher","FaceMatcher","withFaceExpressions","sorted","expressions","asSortedArray","expression","probability","findBestMatch","distance","alert","authenticate"],"mappings":"4HAAAA,EAAAC,EAAAC,GAAAF,EAAA,KAAAA,EAAA,KAIWG,EAAQ,KAChBC,WAAW,OACd,IAAMC,EAAUF,EAAQ,KAExBG,QAAQC,IAAI,CACVF,EAAQG,KAAKC,mBAAmBC,YAAY,YAC5CL,EAAQG,KAAKG,sBAAsBD,YAAY,YAC/CL,EAAQG,KAAKI,kBAAkBF,YAAY,YAC3CL,EAAQG,KAAKK,iBAAiBH,YAAY,cACzCI,KAEH,WAEyC,MADvBC,OAAOC,aACTC,QAAQ,cAOxB,WAEE,IAAMC,EAAaC,SAASC,eAAe,eAC3CC,QAAQC,IAAI,SAEZJ,EAAWK,iBAAiB,SAAU,WAEpC,IAAIC,EAAQnB,EAAQoB,cAAcP,EAAWQ,MAAM,IAEnDF,EAAMV,KAAK,SAACa,GAEVN,QAAQC,IAAI,gBAEZ,IAAIM,EAAYvB,EAAQwB,iBAAiBF,EAAK,IAAItB,EAAQyB,yBAA2BC,mBAAkB,GAAMC,qBAE7GJ,EAAUd,KAAK,SAACmB,GAEdZ,QAAQC,IAAI,oBACZD,QAAQC,IAAIW,EAAIC,YAEhB,IAAIC,EAAOC,KAAKC,UAAUJ,EAAIC,YAC9Bb,QAAQC,IAAI,aACZ,IAAIgB,EAAYvB,OAAOC,aACvBsB,EAAUC,QAAQ,aAAcJ,GAChCpB,OAAOyB,SAASC,eA9BpBC,GAqCJ,WAEE,IAAMC,EAAcxB,SAASC,eAAe,eACtCwB,EAAkB,CAAC,UAAW,QAAS,MAAO,QAAS,aAE7DvB,QAAQC,IAAI,cACZ,IACIuB,EADY9B,OAAOC,aACAC,QAAQ,cAC3B6B,EAAe,GACfC,EAAkBX,KAAKY,MAAMH,GACjC,IAAK,IAAII,KAAKF,EACZD,EAAaI,KAAKH,EAAgBE,IAEpC,IAAIE,EAAaC,aAAaC,KAAKP,IACnCA,EAAe,IACFI,KAAKC,GAElB,IAAIG,EAA0B,IAAIjD,EAAQkD,uBAAuB,OAAQT,GACnEU,EAAc,IAAInD,EAAQoD,YAAYH,EAAwB,IAEpEjC,QAAQC,IAAI,UAGZD,QAAQC,IAAI,sDAAuDsB,EADxD,IAGXD,EAAYpB,iBAAiB,SAAU,WACrC,IAAIC,EAAQnB,EAAQoB,cAAckB,EAAYjB,MAAM,IAEpDF,EAAMV,KAAK,SAACa,GAEV,IAAMC,EAAYvB,EAAQwB,iBAAiBF,EAAK,IAAItB,EAAQyB,yBAA2BC,mBAAkB,GAAM2B,sBAAsB1B,qBAErIJ,EAAUd,KAAK,SAACmB,GAEd,IACI0B,EAAS1B,EAAI2B,YAAYC,gBAE7B,GADAxC,QAAQC,IAAIqC,EAAO,GAAGG,YAClBH,EAAO,GAAGI,aAHQ,IAGwBJ,EAAO,GAAGG,aAAelB,EAflE,GAgBL,CACE,IAAMC,EAASW,EAAYQ,cAAc/B,EAAIC,YACvC+B,EAAWpB,EAAM,SACnBoB,GAAY,GAEdC,MAAM,gBAINA,MAAM,uCAKRA,MAAM,8BAvFZC","file":"static/js/main.9b3910ba.chunk.js","sourcesContent":["// import l10n.js first\nimport 'kaios-gaia-l10n';\nimport './index.css';\n\nconst tf = require('@tensorflow/tfjs')\ntf.setBackend('cpu')\nconst faceapi = require('@vladmandic/face-api/dist/face-api.node-cpu.js')\n\nPromise.all([\n  faceapi.nets.faceRecognitionNet.loadFromUri('./models'),\n  faceapi.nets.faceLandmark68TinyNet.loadFromUri('./models'),\n  faceapi.nets.faceExpressionNet.loadFromUri('./models'),\n  faceapi.nets.tinyFaceDetector.loadFromUri('./models')\n]).then(start);\n\nfunction start() {\n  var myStorage = window.localStorage\n  if (myStorage.getItem('descriptor') == null) {\n    enrol()\n  } else {\n    authenticate()\n  }\n}\n\nfunction enrol() {\n\n  const imageEnrol = document.getElementById('imageUpload');\n  console.log('Ready')\n\n  imageEnrol.addEventListener('change', () => {\n\n    var image = faceapi.bufferToImage(imageEnrol.files[0])\n    \n    image.then((img) => {\n\n      console.log('Image loaded')\n\n      var detection = faceapi.detectSingleFace(img, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks(true).withFaceDescriptor()\n\n      detection.then((dtn) => {\n\n        console.log(\"Detection loaded\")\n        console.log(dtn.descriptor)\n\n        var data = JSON.stringify(dtn.descriptor)\n        console.log('saving...')\n        var myStorage = window.localStorage\n        myStorage.setItem('descriptor', data)\n        window.location.reload()\n\n      });\n    });\n  })\n}\n\nfunction authenticate() {\n\n  const imageUpload = document.getElementById('imageUpload');\n  const expressionArray = ['neutral', 'happy', 'sad', 'angry', 'surprised'];\n\n  console.log('loading...')\n  var myStorage = window.localStorage\n  var result = myStorage.getItem('descriptor')\n  var descriptions = []\n  var descriptorArray = JSON.parse(result)\n  for (let x in descriptorArray) {\n    descriptions.push(descriptorArray[x])\n  }\n  var floatArray = Float32Array.from(descriptions)\n  descriptions = []\n  descriptions.push(floatArray)\n\n  var labeledFaceDescriptors =  new faceapi.LabeledFaceDescriptors('john', descriptions)\n  const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.4)\n\n  console.log('Loaded')\n  // rand = Math.floor(Math.random() * 5)\n  var rand = 4\n  console.log(\"Please upload an image with the following emotion: \"+ expressionArray[rand])\n\n  imageUpload.addEventListener('change', () => {\n    var image = faceapi.bufferToImage(imageUpload.files[0])\n    \n    image.then((img) => {\n\n      const detection = faceapi.detectSingleFace(img, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks(true).withFaceExpressions().withFaceDescriptor()\n\n      detection.then((dtn) => {\n\n        const minConfidence = 0.4\n        var sorted = dtn.expressions.asSortedArray()\n        console.log(sorted[0].expression)\n        if (sorted[0].probability >= minConfidence && sorted[0].expression === expressionArray[rand])\n        {\n          const result = faceMatcher.findBestMatch(dtn.descriptor)\n          const distance = result['distance']\n          if (distance <= 0.4) \n          {\n            alert('You are John')\n          } \n          else\n          {\n            alert('you are not John. \\ntry again.')\n          }\n        }\n        else \n        {\n          alert(\"Liveness Test Failed\")\n        }\n      });\n    });\n  })\n}\n\nfunction useCamera() {\n  var options = {}\n  var pictureOptions = {\n    pictureSize: null,\n    fileFormat: null\n  }\n  \n  var camera = navigator.mozCameras.getListOfCameras()[1]\n\n  function onPictureTaken(blob) {\n    console.log(blob)\n    var blobURL = URL.createObjectURL(blob)\n    var image = new Image()\n    image.src = blobURL\n    console.log(image)\n    document.body.style.backgroundImage = \"url('\" + blobURL + \"')\"\n  }\n\n  function onPictureNotTaken(error) {\n    console.warn(error)\n  }\n  \n  function onSuccess(cameraObj) {\n    var cameraControl = cameraObj.camera\n    pictureOptions.pictureSize = cameraControl.capabilities.pictureSizes[0]\n    pictureOptions.fileformat  = cameraControl.capabilities.fileFormats[0]\n  \n    console.log(cameraControl)\n    cameraControl.takePicture(pictureOptions).then(onPictureTaken, onPictureNotTaken)\n  }\n  \n  function onError(error) {\n    console.warn(error);\n  }\n  \n  navigator.mozCameras.getCamera(camera, options).then(onSuccess, onError);\n}"],"sourceRoot":""}